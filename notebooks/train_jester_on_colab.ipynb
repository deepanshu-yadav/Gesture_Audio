{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "mAf6dSWiM7Pl",
    "outputId": "f941eacf-9ecc-4c63-b538-b093a360d6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_lTzYlzNo5t"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/My\\ Drive/jester_dataset.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "JtYbTW4BN0hp",
    "outputId": "2198a7c1-0eeb-4c05-febc-d3f3703d7e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  jester_dataset.zip\n",
      "   creating: jester_dataset/\n",
      "  inflating: jester_dataset/jester_X_train.npy  \n",
      "  inflating: jester_dataset/jester_y_train.npy  \n",
      "  inflating: jester_dataset/jester_X_valid.npy  \n",
      "  inflating: jester_dataset/jester_y_valid.npy  \n"
     ]
    }
   ],
   "source": [
    "!unzip jester_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DugnKWmNOA3G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('jester_dataset/jester_X_train.npy')\n",
    "y_train = np.load('jester_dataset/jester_y_train.npy')\n",
    "X_valid = np.load('jester_dataset/jester_X_valid.npy')\n",
    "y_valid = np.load('jester_dataset/jester_y_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBrIw8njOvU7"
   },
   "outputs": [],
   "source": [
    "X_train_fp= X_train.astype(np.float16)\n",
    "y_train_int= y_train.astype(np.uint8)\n",
    "X_valid_fp= X_valid.astype(np.float16)\n",
    "y_valid_int= y_valid.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE0ZJ9vBOyRi"
   },
   "outputs": [],
   "source": [
    "del X_train\n",
    "del y_train\n",
    "del X_valid\n",
    "del y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "BHzqeeRKQuOU",
    "outputId": "e4ea9186-ca4d-4762-8873-c2a1c3fd8f05"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiTkxgnHRDAL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDAHbgQCRJZx"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_images_train  = np.array(scaler.fit_transform(X_train_fp.reshape(-1, 30*64*64)))\n",
    "scaled_images_train  = np.array(scaled_images_train.reshape(-1, 30, 64, 64, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptxllV-CSV0z"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_images_valid  = np.array(scaler.fit_transform(X_valid_fp.reshape(-1, 30*64*64)))\n",
    "scaled_images_valid  = np.array(scaled_images_valid.reshape(-1, 30, 64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0St8ywiLTYGR"
   },
   "outputs": [],
   "source": [
    "del X_train_fp\n",
    "del X_valid_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDPLR7mJTefx"
   },
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(6, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VH0IO3ixIAUr"
   },
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vV2ljEwEIASB"
   },
   "outputs": [],
   "source": [
    "# choose the loss and optimizer methods\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oc7vHL4YIAPr"
   },
   "outputs": [],
   "source": [
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_today/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "r3WaVGOwIANf",
    "outputId": "3c96d74f-2161-4d3e-eed1-53e48511299f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 25876 samples, validate on 3179 samples\n",
      "Epoch 1/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.6322 - acc: 0.7678\n",
      "Epoch 00001: saving model to training_today/cp-0001.ckpt\n",
      "25876/25876 [==============================] - 92s 4ms/sample - loss: 0.6321 - acc: 0.7679 - val_loss: 0.3442 - val_acc: 0.8849\n",
      "Epoch 2/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8819\n",
      "Epoch 00002: saving model to training_today/cp-0002.ckpt\n",
      "25876/25876 [==============================] - 85s 3ms/sample - loss: 0.3469 - acc: 0.8819 - val_loss: 0.2560 - val_acc: 0.9166\n",
      "Epoch 3/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9253\n",
      "Epoch 00003: saving model to training_today/cp-0003.ckpt\n",
      "25876/25876 [==============================] - 84s 3ms/sample - loss: 0.2253 - acc: 0.9253 - val_loss: 0.2079 - val_acc: 0.9333\n",
      "Epoch 4/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9479\n",
      "Epoch 00004: saving model to training_today/cp-0004.ckpt\n",
      "25876/25876 [==============================] - 84s 3ms/sample - loss: 0.1584 - acc: 0.9479 - val_loss: 0.1804 - val_acc: 0.9437\n",
      "Epoch 5/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9648\n",
      "Epoch 00005: saving model to training_today/cp-0005.ckpt\n",
      "25876/25876 [==============================] - 84s 3ms/sample - loss: 0.1039 - acc: 0.9649 - val_loss: 0.2166 - val_acc: 0.9399\n",
      "Epoch 6/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9740\n",
      "Epoch 00006: saving model to training_today/cp-0006.ckpt\n",
      "25876/25876 [==============================] - 85s 3ms/sample - loss: 0.0741 - acc: 0.9740 - val_loss: 0.2288 - val_acc: 0.9399\n",
      "Epoch 7/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9849\n",
      "Epoch 00007: saving model to training_today/cp-0007.ckpt\n",
      "25876/25876 [==============================] - 85s 3ms/sample - loss: 0.0457 - acc: 0.9849 - val_loss: 0.2520 - val_acc: 0.9396\n",
      "Epoch 8/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9848\n",
      "Epoch 00008: saving model to training_today/cp-0008.ckpt\n",
      "25876/25876 [==============================] - 84s 3ms/sample - loss: 0.0441 - acc: 0.9848 - val_loss: 0.2324 - val_acc: 0.9528\n",
      "Epoch 9/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9882\n",
      "Epoch 00009: saving model to training_today/cp-0009.ckpt\n",
      "25876/25876 [==============================] - 85s 3ms/sample - loss: 0.0347 - acc: 0.9883 - val_loss: 0.2341 - val_acc: 0.9481\n",
      "Epoch 10/10\n",
      "25856/25876 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9913\n",
      "Epoch 00010: saving model to training_today/cp-0010.ckpt\n",
      "25876/25876 [==============================] - 85s 3ms/sample - loss: 0.0260 - acc: 0.9913 - val_loss: 0.2475 - val_acc: 0.9525\n"
     ]
    }
   ],
   "source": [
    "# Run the training \n",
    "history = model.fit(scaled_images_train, y_train_int,\n",
    "                    callbacks = [cp_callback],\n",
    "                    validation_data=(scaled_images_valid, y_valid_int),\n",
    "                    batch_size=32,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "rjjJqPsVLyUZ",
    "outputId": "d1251e68-5b07-4f9e-9f72-ee9beae4893b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.7679317,\n",
       "  0.88185966,\n",
       "  0.92529756,\n",
       "  0.9479054,\n",
       "  0.9648709,\n",
       "  0.97399133,\n",
       "  0.98492813,\n",
       "  0.9847735,\n",
       "  0.9882517,\n",
       "  0.9913047],\n",
       " 'loss': [0.6321346970057078,\n",
       "  0.34694595218239177,\n",
       "  0.2252556763268048,\n",
       "  0.15839191958190743,\n",
       "  0.1039202843096944,\n",
       "  0.07406272675567357,\n",
       "  0.04573429997320507,\n",
       "  0.044083863178289384,\n",
       "  0.034707821390872404,\n",
       "  0.02596139433367995],\n",
       " 'val_acc': [0.88486946,\n",
       "  0.91664046,\n",
       "  0.93331236,\n",
       "  0.943693,\n",
       "  0.9399182,\n",
       "  0.9399182,\n",
       "  0.9396036,\n",
       "  0.95281535,\n",
       "  0.9480969,\n",
       "  0.95250076],\n",
       " 'val_loss': [0.3442333872896127,\n",
       "  0.25604453895237506,\n",
       "  0.20794550315853408,\n",
       "  0.180431334266967,\n",
       "  0.2165728809043048,\n",
       "  0.22881977231285414,\n",
       "  0.2520445537580263,\n",
       "  0.2324329875775804,\n",
       "  0.23407297408796993,\n",
       "  0.24750294937485154]}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irJQP9P7IAKE"
   },
   "outputs": [],
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/3d-cnn-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lT8dLh4JIAHH",
    "outputId": "72085a22-206a-4907-90e9-0e3f6f2d4ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d-cnn-basic.data-00000-of-00002  3d-cnn-basic.index\n",
      "3d-cnn-basic.data-00001-of-00002  checkpoint\n"
     ]
    }
   ],
   "source": [
    "!ls weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "xnY8rtwGRueO",
    "outputId": "e99f1738-a3f3-4be7-f731-5497e9a7b635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: weights/ (stored 0%)\n",
      "  adding: weights/3d-cnn-basic.data-00000-of-00002 (deflated 8%)\n",
      "  adding: weights/checkpoint (deflated 38%)\n",
      "  adding: weights/3d-cnn-basic.data-00001-of-00002 (deflated 81%)\n",
      "  adding: weights/3d-cnn-basic.index (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r weights.zip weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7Vg9BrPTeWm"
   },
   "outputs": [],
   "source": [
    "!cp weights.zip /content/drive/My\\ Drive/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTnbCn1oIAC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7m88o4hQRaY0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hEQnhm5LOAs"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(6, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bek5ttdcLt8S"
   },
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wru2BxZhMMRq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvWgZUXtL-cn"
   },
   "outputs": [],
   "source": [
    "# use tensorflow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_train, y_train_fp))\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_valid, y_valid_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rG8z-rVTMjFV"
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqE3i0WaM8yC"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pKPPeSTNLou"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(image)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNrTrvFrNhW3"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(image, targets):\n",
    "    predictions = model(image)\n",
    "    t_loss = loss_fn(targets, predictions)\n",
    "    # Set the metrics for the test\n",
    "    valid_loss(t_loss)\n",
    "    valid_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxDzbg7aNhop"
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'training_checkpoints/tf_ckpts', max_to_keep=10)\n",
    "ckpt.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7pbs7dsNh7U"
   },
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 32\n",
    "b = 0\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for epoch in range(epoch):\n",
    "    # Training set\n",
    "    for images_batch, targets_batch in train_dataset.batch(batch_size):\n",
    "        train_step(images_batch, targets_batch)\n",
    "        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "        print(template.format(\n",
    "            b, len(training_targets), train_loss.result(), \n",
    "            train_accuracy.result()*100\n",
    "        ), end=\"\")\n",
    "        b += batch_size\n",
    "    # Validation set\n",
    "    for images_batch, targets_batch in cv_dataset.batch(batch_size):\n",
    "        valid_step(images_batch, targets_batch)\n",
    "\n",
    "    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch+1,\n",
    "        valid_loss.result(), \n",
    "        valid_accuracy.result()*100)\n",
    "    )\n",
    "    training_acc.append(float(train_accuracy.result()*100))\n",
    "    validation_acc.append(float(valid_accuracy.result()*100))\n",
    "    ckpt.step.assign_add(1)\n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJ2OxQrVOBiH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "train_jester.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
